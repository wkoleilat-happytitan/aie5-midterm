{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 86 documents\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n",
      "Loaded: data/Stripe API Reference.pdf\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "path = \"data/\"\n",
    "text_loader = DirectoryLoader(path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
    "\n",
    "# Load documents\n",
    "training_documents = text_loader.load()\n",
    "print(f\"Successfully loaded {len(training_documents)} documents\")\n",
    "\n",
    "# Get paths of loaded documents\n",
    "document_paths = [doc.metadata['source'] for doc in training_documents]\n",
    "\n",
    "# Print paths\n",
    "for path in document_paths:\n",
    "    print(f\"Loaded: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 750,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_documents = text_splitter.split_documents(text_loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "id_set = set()\n",
    "\n",
    "for document in training_documents:\n",
    "  id = str(uuid.uuid4())\n",
    "  while id in id_set:\n",
    "    id = uuid.uuid4()\n",
    "  id_set.add(id)\n",
    "  document.metadata[\"id\"] = id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_split_documents = training_documents[:len(training_documents) - 24]\n",
    "val_split_documents = training_documents[len(training_documents) - 24:102-12]\n",
    "test_split_documents = training_documents[102-12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "qa_chat_model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "qa_prompt = \"\"\"\\\n",
    "Given the following context, you must generate questions based on only the provided context.\n",
    "\n",
    "You are to generate {n_questions} questions which should be provided in the following format:\n",
    "\n",
    "1. QUESTION #1\n",
    "2. QUESTION #2\n",
    "...\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt_template = ChatPromptTemplate.from_template(qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generation_chain = qa_prompt_template | qa_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "async def create_questions(documents, n_questions):\n",
    "    questions = {}  # Will store question_id -> question text\n",
    "    relevant_docs = {}  # Will store question_id -> list of relevant doc ids\n",
    "    \n",
    "    # Create a progress bar\n",
    "    for document in tqdm.tqdm(documents, desc=\"Processing documents\"):\n",
    "        # Generate questions for this document\n",
    "        response = await question_generation_chain.ainvoke({\n",
    "            \"context\": document.page_content,\n",
    "            \"n_questions\": n_questions\n",
    "        })\n",
    "        \n",
    "        # Parse the numbered questions from response\n",
    "        response_lines = response.content.strip().split('\\n')\n",
    "        parsed_questions = [line.split('. ', 1)[1] for line in response_lines \n",
    "                          if line and line[0].isdigit()]\n",
    "        \n",
    "        # For each generated question\n",
    "        for question in parsed_questions:\n",
    "            # Generate unique ID for this question\n",
    "            question_id = str(uuid.uuid4())\n",
    "            \n",
    "            # Store the question\n",
    "            questions[question_id] = question\n",
    "            \n",
    "            # Store the document ID this question was generated from\n",
    "            relevant_docs[question_id] = [document.metadata[\"id\"]]\n",
    "    \n",
    "    return questions, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:   0%|          | 0/162 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 162/162 [02:25<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "training_questions, training_relevant_contexts = await create_questions(training_split_documents, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "val_questions, val_relevant_contexts = await create_questions(val_split_documents, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 96/96 [01:31<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "test_questions, test_relevant_contexts = await create_questions(test_split_documents, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "training_corpus = {train_item.metadata[\"id\"] : train_item.page_content for train_item in training_split_documents}\n",
    "\n",
    "train_dataset = {\n",
    "    \"questions\" : training_questions,\n",
    "    \"relevant_contexts\" : training_relevant_contexts,\n",
    "    \"corpus\" : training_corpus\n",
    "}\n",
    "\n",
    "with open(\"training_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(train_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_corpus = {val_item.metadata[\"id\"] : val_item.page_content for val_item in val_split_documents}\n",
    "\n",
    "val_dataset = {\n",
    "    \"questions\" : val_questions,\n",
    "    \"relevant_contexts\" : val_relevant_contexts,\n",
    "    \"corpus\" : val_corpus\n",
    "}\n",
    "\n",
    "with open(\"val_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(val_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = {test_item.metadata[\"id\"] : test_item.page_content for test_item in test_split_documents}\n",
    "\n",
    "test_dataset = {\n",
    "    \"questions\" : test_questions,\n",
    "    \"relevant_contexts\" : test_relevant_contexts,\n",
    "    \"corpus\" : train_corpus\n",
    "}\n",
    "\n",
    "with open(\"test_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(test_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walidkoleilat/aie5-midterm/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_id = \"Snowflake/snowflake-arctic-embed-l\"\n",
    "model = SentenceTransformer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from sentence_transformers import InputExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = train_dataset['corpus']\n",
    "queries = train_dataset['questions']\n",
    "relevant_docs = train_dataset['relevant_contexts']\n",
    "\n",
    "examples = []\n",
    "for query_id, query in queries.items():\n",
    "    doc_id = relevant_docs[query_id][0]\n",
    "    text = corpus[doc_id]\n",
    "    example = InputExample(texts=[query, text])\n",
    "    examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    examples, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
    "\n",
    "matryoshka_dimensions = [768, 512, 256, 128, 64]\n",
    "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
    "train_loss = MatryoshkaLoss(\n",
    "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "\n",
    "corpus = val_dataset['corpus']\n",
    "queries = val_dataset['questions']\n",
    "relevant_docs = val_dataset['relevant_contexts']\n",
    "\n",
    "evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = uv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/yw57p78h?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x34691cd70>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/66 2:38:26 < 3:23:42, 0.00 it/s, Epoch 0.88/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/walidkoleilat/aie5-midterm/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "  File \"/var/folders/xp/fw_gm3jn2xq5ngzrpc74m4lm0000gn/T/ipykernel_33451/189950450.py\", line 3, in <module>\n",
      "    model.fit(\n",
      "    ~~~~~~~~~^\n",
      "        train_objectives=[(loader, train_loss)],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        evaluation_steps=50\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/walidkoleilat/aie5-midterm/.venv/lib/python3.13/site-packages/sentence_transformers/fit_mixin.py\", line 385, in fit\n",
      "    trainer.train()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"/Users/walidkoleilat/aie5-midterm/.venv/lib/python3.13/site-packages/transformers/trainer.py\", line 2241, in train\n",
      "    return inner_training_loop(\n",
      "        args=args,\n",
      "    ...<2 lines>...\n",
      "        ignore_keys_for_eval=ignore_keys_for_eval,\n",
      "    )\n",
      "  File \"/Users/walidkoleilat/aie5-midterm/.venv/lib/python3.13/site-packages/transformers/trainer.py\", line 2548, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "  File \"/Users/walidkoleilat/aie5-midterm/.venv/lib/python3.13/site-packages/transformers/trainer.py\", line 3740, in training_step\n",
      "    self.accelerator.backward(loss, **kwargs)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/walidkoleilat/aie5-midterm/.venv/lib/python3.13/site-packages/accelerate/accelerator.py\", line 2329, in backward\n",
      "  File \"/Users/walidkoleilat/aie5-midterm/.venv/lib/python3.13/site-packages/torch/_tensor.py\", line 626, in backward\n",
      "    torch.autograd.backward(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self, gradient, retain_graph, create_graph, inputs=inputs\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/walidkoleilat/aie5-midterm/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py\", line 347, in backward\n",
      "    _engine_run_backward(\n",
      "    ~~~~~~~~~~~~~~~~~~~~^\n",
      "        tensors,\n",
      "        ^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        accumulate_grad=True,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/walidkoleilat/aie5-midterm/.venv/lib/python3.13/site-packages/torch/autograd/graph.py\", line 823, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        t_outputs, *args, **kwargs\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )  # Calls into the C++ engine to run the backward pass\n",
      "    ^\n",
      "RuntimeError: MPS backend out of memory (MPS allocated: 16.80 GB, other allocations: 3.59 GB, max allowed: 20.40 GB). Tried to allocate 16.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/walidkoleilat/aie5-midterm/.venv/lib/python3.13/site-packages/pygments/styles/__init__.py\", line 45, in get_style_by_name\n",
      "ModuleNotFoundError: No module named 'pygments.styles.default'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/walidkoleilat/aie5-midterm/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 2170, in showtraceback\n",
      "  File \"/Users/walidkoleilat/aie5-midterm/.venv/lib/python3.13/site-packages/IPython/core/ultratb.py\", line 1457, in structured_traceback\n",
      "  File \"/Users/walidkoleilat/aie5-midterm/.venv/lib/python3.13/site-packages/IPython/core/ultratb.py\", line 1348, in structured_traceback\n",
      "  File \"/Users/walidkoleilat/aie5-midterm/.venv/lib/python3.13/site-packages/IPython/core/ultratb.py\", line 1195, in structured_traceback\n",
      "  File \"/Users/walidkoleilat/aie5-midterm/.venv/lib/python3.13/site-packages/IPython/core/ultratb.py\", line 1085, in format_exception_as_a_whole\n",
      "  File \"/Users/walidkoleilat/aie5-midterm/.venv/lib/python3.13/site-packages/IPython/core/ultratb.py\", line 1136, in get_records\n",
      "  File \"/Users/walidkoleilat/aie5-midterm/.venv/lib/python3.13/site-packages/pygments/styles/__init__.py\", line 47, in get_style_by_name\n",
      "pygments.util.ClassNotFound: Could not find style module 'pygments.styles.default', though it should be builtin.\n"
     ]
    }
   ],
   "source": [
    "warmup_steps = int(len(loader) * EPOCHS * 0.1)\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(loader, train_loss)],\n",
    "    epochs=EPOCHS,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path='finetuned_arctic_ft',\n",
    "    show_progress_bar=True,\n",
    "    evaluator=evaluator,\n",
    "    evaluation_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_username = \"wkoleilat-happytitan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(f\"{hf_username}/build-or-buy-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_openai(\n",
    "    dataset,\n",
    "    embed_model,\n",
    "    top_k=5,\n",
    "    verbose=False,\n",
    "):\n",
    "  corpus = dataset['corpus']\n",
    "  questions = dataset['questions']\n",
    "  relevant_docs = dataset['relevant_contexts']\n",
    "  documents = [Document(page_content=content, metadata={\"id\": doc_id}) for doc_id, content in corpus.items()]\n",
    "  vectorstore = FAISS.from_documents(documents, embed_model)\n",
    "\n",
    "  retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
    "\n",
    "  eval_results = []\n",
    "  for id, question in tqdm(questions.items()):\n",
    "    retrieved_nodes = retriever.invoke(question)\n",
    "    retrieved_ids = [node.metadata[\"id\"] for node in retrieved_nodes]\n",
    "    expected_id = relevant_docs[id][0]\n",
    "    is_hit = expected_id in retrieved_ids\n",
    "    eval_results.append({\"id\": id, \"question\": question, \"expected_id\": expected_id, \"is_hit\": is_hit})\n",
    "\n",
    "  return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te3_openai = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "te3_results = evaluate_openai(test_dataset, te3_openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te3_results_df = pd.DataFrame(te3_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te3_hit_rate = te3_results_df[\"is_hit\"].mean()\n",
    "te3_hit_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "huggingface_embeddings = HuggingFaceEmbeddings(model_name=\"Snowflake/snowflake-arctic-embed-l\")\n",
    "arctic_embed_m_results = evaluate_openai(test_dataset, huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arctic_embed_m_results_df = pd.DataFrame(arctic_embed_m_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arctic_embed_m_hit_rate = arctic_embed_m_results_df[\"is_hit\"].mean()\n",
    "arctic_embed_m_hit_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_embeddings = HuggingFaceEmbeddings(model_name=\"finetuned_arctic_ft\")\n",
    "finetune_results = evaluate_openai(test_dataset, finetune_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_results_df = pd.DataFrame(finetune_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_hit_rate = finetune_results_df[\"is_hit\"].mean()\n",
    "finetune_hit_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 600,\n",
    "    chunk_overlap  = 50,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "training_documents = text_splitter.split_documents(text_loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "base_vectorstore = FAISS.from_documents(training_documents, huggingface_embeddings)\n",
    "base_retriever = base_vectorstore.as_retriever(search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_PROMPT = \"\"\"\\\n",
    "Given a provided context and a question, you must answer the question. If you do not know the answer, you must state that you do not know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt_template = ChatPromptTemplate.from_template(RAG_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_llm =  ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "base_rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt_template | rag_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_rag_chain.invoke({\"question\" : \"What is an agent?\"})[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_vectorstore = FAISS.from_documents(training_documents, finetune_embeddings)\n",
    "finetune_retriever = finetune_vectorstore.as_retriever(search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | finetune_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt_template | rag_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_rag_chain.invoke({\"question\" : \"What is an Agent?\"})[\"response\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
